defaults:

  # Train Script
  logdir: /dev/null
  seed: 0
  task: dmc_walker_walk
  envs: 1
  envs_parallel: none
  discrete: False
  render_size: [ 64, 64 ]
  dmc_camera: -1
  atari_grayscale: True
  time_limit: 0
  action_repeat: 1
  steps: 1e8
  log_every: 1e4
  eval_every: 1e5
  save_every: 1e7
  eval_eps: 1
  prefill: 10000
  pretrain: 1
  train_every: 5
  train_steps: 1
  expl_until: 0
  replay: { capacity: 2e6, ongoing: False, minlen: 50, maxlen: 50, prioritize_ends: True }
  dataset: { batch: 16, length: 50 }
  log_keys_video: [ 'image' ]
  log_keys_sum: '^$'
  log_keys_mean: '^$'
  log_keys_max: '^$'
  precision: 16
  jit: True
  make_graph: False
  benchmark: False

  use_wandb: False
  wandb_config: {
    project: project,
    entity: entity,
    job_type: job_type,
    name: name,
    tags: [ 'tag' ]
  }

  # Agent
  clip_rewards: identity
  expl_behavior: greedy
  expl_noise: 0.0
  eval_noise: 0.0
  eval_state_mean: False
  no_behavior_training: False

  # World Model
  grad_heads: [ decoder, reward, discount ]
  pred_discount: True
  use_int_reward: False
  int_reward_sources: [ expl ]
  int_reward_coef: { expl: 0.0002, attention: 0.5 }
  use_head_mask: False
  head_mask: { gradient: False }
  int_reward_scales: { reward: 1.0, image: 1.0, discount: 0.0 }
  use_raw_input_in_transformer: False
  use_independent_transformer: False
  use_inside_transformer: False
  use_independent_transformer_encoder: False
  use_transformer_reward: False
  include_transformer_embed: False
  future_importance_source: reward
  rssm: { use_transformer: False, ensemble: 1, hidden: 1024, deter: 1024, stoch: 32, discrete: 32, act: elu, norm: none, std_act: sigmoid2, min_std: 0.1, 
          exclude_deter_feat: False, transformer: { version: 1, d_model: 1024, num_layers: 1, num_heads: 8, memory_size: 256, pe_input: 10000, dff: 1024, 
            no_pe: False, reverse_pe: False },
          inside_transformer: { d_model: 1024, num_layers: 1, num_heads: 8, memory_size: 10, pe_input: 10000, dff: 1024 },
          use_forward_loss: False, use_transformer_reward_head: False }
  encoder: { mlp_keys: '$^', cnn_keys: '^image$', act: elu, norm: none, cnn_depth: 48, cnn_kernels: [ 4, 4, 4, 4 ], mlp_layers: [ 400, 400, 400, 400 ] }
  decoder: { mlp_keys: '$^', cnn_keys: '^image$', act: elu, norm: none, cnn_depth: 48, cnn_kernels: [ 5, 5, 6, 6 ], mlp_layers: [ 400, 400, 400, 400 ] }
  reward_head: { layers: 4, units: 400, act: elu, norm: none, dist: mse }
  discount_head: { layers: 4, units: 400, act: elu, norm: none, dist: binary }
  loss_scales: { kl: 1.0, reward: 1.0, int_reward_expl: 1.0, int_reward_attention: 1.0, discount: 1.0, proprio: 1.0, image: 1.0 }
  kl: { free: 0.0, forward: False, balance: 0.8, free_avg: True }
  model_opt: { opt: adam, lr: 1e-4, eps: 1e-5, clip: 100, wd: 1e-6 }
  bootstrap_frames: 20
  video_pred_batches: 6

  reward_pred: { transformer: { d_model: 256, num_layers: 2, num_heads: 4, pe_input: 10000, dff: 512 } }
  reward_pred_opt: { opt: adam, lr: 1e-4, eps: 1e-5, clip: 100, wd: 1e-6 }
  reward_pred_dataset: { batch: 16, length: 50 }

  # Actor Critic
  actor: { layers: 4, units: 400, act: elu, norm: none, dist: trunc_normal, min_std: 0.1 }
  critic: { layers: 4, units: 400, act: elu, norm: none, dist: mse }
  actor_opt: { opt: adam, lr: 8e-5, eps: 1e-5, clip: 100, wd: 1e-6 }
  critic_opt: { opt: adam, lr: 2e-4, eps: 1e-5, clip: 100, wd: 1e-6 }
  discount: 0.99
  discount_lambda: 0.95
  imag_horizon: 15
  actor_grad: dynamics
  actor_grad_mix: 0.1
  actor_ent: 3e-3
  slow_target: True
  slow_target_update: 100
  slow_target_fraction: 1
  slow_baseline: True
  reward_norm: { momentum: 1.0, scale: 1.0, eps: 1e-8 }

  # Exploration
  expl_intr_scale: 1.0
  expl_extr_scale: 0.0
  expl_opt: { opt: adam, lr: 3e-4, eps: 1e-5, clip: 100, wd: 1e-6 }
  expl_head: { layers: 4, units: 400, act: elu, norm: none, dist: mse }
  expl_reward_norm: { momentum: 1.0, scale: 1.0, eps: 1e-8 }
  disag_target: stoch
  disag_log: False
  disag_models: 10
  disag_offset: 1
  disag_action_cond: True
  expl_model_loss: kl

atari:

  task: atari_pong
  time_limit: 27000
  discrete: True
  action_repeat: 4
  steps: 5e7
  eval_every: 2.5e5
  log_every: 1e4
  save_every: 5e6
  prefill: 50000
  train_every: 16
  clip_rewards: tanh
  rssm: { hidden: 600, deter: 600 }
  actor.dist: onehot
  model_opt.lr: 2e-4
  actor_opt.lr: 4e-5
  critic_opt.lr: 1e-4
  actor_ent: 1e-3
  discount: 0.999
  actor_grad: reinforce
  actor_grad_mix: 0
  loss_scales.kl: 0.1
  loss_scales.discount: 5.0

crafter:

  task: crafter_reward
  discrete: True
  log_keys_max: '^log_achievement_.*'
  log_keys_sum: '^log_reward$'
  log_every: 1e4
  eval_every: 1e5
  prefill: 10000
  train_every: 5
  rssm: { hidden: 1024, deter: 1024 }
  dataset.batch: 16
  actor.dist: onehot
  model_opt.lr: 1e-4
  actor_opt.lr: 8e-5
  critic_opt.lr: 2e-4
  actor_ent: 2e-3
  discount: 0.999
  actor_grad: reinforce
  .*\.norm: layer

dmc:

  task: dmc_walker_walk
  action_repeat: 2
  eval_every: 1e4
  log_every: 1e4
  prefill: 1000
  train_every: 5
  pretrain: 100
  pred_discount: False
  grad_heads: [ decoder, reward ]
  rssm: { hidden: 200, deter: 200 }
  model_opt.lr: 3e-4
  actor_opt.lr: 8e-5
  critic_opt.lr: 8e-5
  actor_ent: 1e-4
  discount: 0.99
  actor_grad: dynamics
  kl.free: 1.0
  replay.prioritize_ends: False

minigrid:

  render_size: [ 56, 56 ]
  task: minigrid
  discrete: True
  steps: 1e6
  eval_every: 1e4
  log_every: 1e4
  save_every: 2e5
  prefill: 10000
  train_every: 16
  clip_rewards: tanh
  rssm: { hidden: 200, deter: 200 }
  decoder: { cnn_kernels: [ 4, 5, 6, 6 ] }
  replay: { minlen: 1 }
  actor.dist: onehot
  model_opt.lr: 2e-4
  actor_opt.lr: 4e-5
  critic_opt.lr: 1e-4
  actor_ent: 1e-3
  discount: 0.99
  actor_grad: reinforce
  actor_grad_mix: 0
  loss_scales.kl: 0.1
  loss_scales.discount: 5.0

debug:

  rssm: { hidden: 64, deter: 64, stoch: 4, discrete: 4, transformer: { num_layers: 2, num_heads: 8, pe_input: 1000, dff: 128 } }
  encoder: { cnn_depth: 8, cnn_kernels: [ 4, 4 ], mlp_layers: [ 50, 50 ] }
  decoder: { cnn_depth: 8, cnn_kernels: [ 5, 6 ], mlp_layers: [ 50, 50 ] }
  reward_head: { layers: 2, units: 50 }
  discount_head: { layers: 2, units: 50 }

  jit: False
  time_limit: 100
  eval_every: 300
  log_every: 300
  prefill: 500
  pretrain: 1
  train_steps: 1
  replay: { minlen: 10, maxlen: 30 }
  dataset: { batch: 10, length: 10 }

replay:

  steps: 1800
  dataset: { batch: 6, length: 300 }